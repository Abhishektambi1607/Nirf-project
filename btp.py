# -*- coding: utf-8 -*-
"""BTP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11sVfOinpBb9Y03ZivVGPzYEpK9QVyZqm
"""

!pip install pdfplumber

!pip install tabula-py

import os
import pdfplumber
import re
import pandas as pd
import tabula
import numpy as np
import matplotlib.pyplot as plt

# Directory containing your PDF files
pdf_directory = "/content/drive/MyDrive/college_pdf"

from google.colab import drive
drive.mount('/content/drive')

# Initialize lists to store total values and file names
all_nt_values = []
all_ne_values = []
file_names = []

# Loop through PDF files in the directory
for filename in os.listdir(pdf_directory):
    if filename.endswith('.pdf'):
        pdf_path = os.path.join(pdf_directory, filename)

        try:
            # Use tabula to extract tables from the PDF
            tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)

            # Assuming tables[0] contains '2020-21' column and tables[1] contains 'Total Students' column
            total_nt = tables[0]['2021-22'].sum()
            total_ne = tables[1]['Total Students'].sum()

            # Append the values to the lists
            all_nt_values.append(total_nt)
            all_ne_values.append(total_ne)
            file_names.append(filename)

        except Exception as e:
            print(f"Error processing {filename}: {e}")

# Create a single DataFrame with 'NT', 'NE', and 'File Name' columns
combined_dataframe = pd.DataFrame({'NT': all_nt_values, 'NE': all_ne_values, 'File Name': file_names})

# Display the combined DataFrame
print("Combined DataFrame:")
combined_dataframe

combined_dataframe

# Navigate to the folder where your PDF files are located
pdf_folder_path = '/content/drive/MyDrive/college_pdf'
os.chdir(pdf_folder_path)

# List PDF files
pdf_files = [file for file in os.listdir() if file.endswith('.pdf')]

# Initialize a list to store extracted data
extracted_data = []

# Access and process PDF files
for pdf_file_path in pdf_files:
    with pdfplumber.open(pdf_file_path) as pdf:
        # Extract text from the first three pages
        for page_number in range(1, 4):
            current_page = pdf.pages[page_number - 1]
            text = current_page.extract_text()

            # Example: Extracting total students from the text
            total_students_match = re.search(r'Total Students\nFull Time (\d+)\nPart Time (\d+)', text)
            if total_students_match:
                full_time_students = int(total_students_match.group(1))
                part_time_students = int(total_students_match.group(2))
                extracted_data.append({
                    'file': pdf_file_path,
                    'page': page_number,
                    'full_time_students': full_time_students,
                    'part_time_students': part_time_students,
                })

# Print the extracted data
extracted_data

extracted_data_df = pd.DataFrame(extracted_data)

# Sum the columns using the + operator
extracted_data_df['NP'] = extracted_data_df['full_time_students'] + extracted_data_df['part_time_students']
extracted_data_df = pd.concat([extracted_data_df, combined_dataframe], axis=1)

# Remove columns 'B' and 'C'
columns_to_remove = ['full_time_students', 'part_time_students', 'file', 'page']
extracted_data_df = extracted_data_df.drop(columns=columns_to_remove)

# Display the DataFrame after removing columns
print("\nDataFrame after removing columns:")
extracted_data_df

df1 = pd.DataFrame({'SS': [18.50, 18.43, 18.50, 18.50, 17.61, 18.45, 18.50, 12.65, 17.50, 17.35, 20.00, 17.50, 17.59, 10.88, 16.80, 17.50, 15.00, 10.00, 18.00, 17.16, 16.50, 11.05, 16.00, 10.82, 18.84, 10.50, 17.36, 17.55, 18.90, 11.04, 15.50, 11.00, 9.24, 16.47, 13.44, 16.47, 15.25, 18.95, 20.00, 13.34, 10.99, 14.04, 14.42, 17.43, 12.50, 15.59, 10.95, 7.19, 16.01, 18.00, 6.79, 7.11, 12.52, 14.50, 9.50, 12.86, 5.00, 15.11, 9.32, 16.21, 16.34, 14.00, 14.71, 15.50, 14.05, 15.46, 8.08, 11.49, 7.56, 14.08, 2.89, 5.81, 11.50, 6.16, 11.42, 12.97, 11.89, 13.75, 8.00, 15.45, 6.59, 13.01, 14.00, 9.50, 13.85, 14.30, 13.10, 6.43, 10.38, 5.84, 12.40, 14.43, 7.00, 12.50, 7.15, 14.00, 8.83, 10.93, 7.89, 14.30]})

# Concatenate the two DataFrames vertically (along rows)
df = pd.concat([df1, extracted_data_df], axis=1)

"""# Display the horizontally combined DataFrame"""

df

from matplotlib import pyplot as plt
df.plot(kind='scatter', x='NT', y='SS', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
df.plot(kind='scatter', x='NE', y='SS', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
df.plot(kind='scatter', x='NP', y='SS', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

import matplotlib.pyplot as plt

x = df['NE']
y = df['SS']

# depict first scatted plot
plt.scatter(x, y, c='green')

# second data point
x = df['NP']
y = df['SS']

# depict second scatted plot
plt.scatter(x, y, c='red')

x = df['NT']
y = df['SS']

# depict second scatted plot
plt.scatter(x, y, c='blue')
plt.legend()
# depict illustration
plt.show()

"""#Train, validation, test datasets"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Assuming your DataFrame is already defined and contains the necessary columns
# Here, 'NT', 'NE', 'NP', and 'SS' are column names

# Split the data into features (X) and target variable (y)
X = df[['NT', 'NE', 'NP']]
y = df['SS']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
model = DecisionTreeRegressor()

# Train the model
reg = model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Now, you can use the trained model to make predictions on new data
# For example, if you have new values for NT, NE, and NP
new_data = pd.DataFrame({'NT': [676], 'NE': [1386], 'NP': [305]})
predicted_ss = model.predict(new_data)

print(f'Predicted SS: {predicted_ss[0]}')

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have the actual and predicted values in y_test and y_pred

# Get indices for sorting
indices = np.argsort(y_test)

# Sort actual and predicted values based on y_test
y_test_sorted = np.array(y_test)[indices]
y_pred_sorted = np.array(y_pred)[indices]

# Create an array for the x-axis ticks
x_ticks = np.arange(len(y_test))

# Plotting
plt.figure(figsize=(10, 6))

# Bar plot for actual values
plt.bar(x_ticks - 0.2, y_test_sorted, width=0.4, label='Actual', alpha=0.7)

# Bar plot for predicted values
plt.bar(x_ticks + 0.2, y_pred_sorted, width=0.4, label='Predicted', alpha=0.7)

# Hide x-axis values
plt.xticks([])

# Adding labels and title
# plt.xlabel('Sample Index')
plt.ylabel('Target Variable (SS)')
plt.title('Actual vs Predicted Values')
plt.legend()

# Show the plot
plt.show()

"""#Calulate Median Salary(GMS) function"""

import tabula
import os
import pandas as pd
import re

# Initialize lists to store total values and file names
all_median_salary = []
file_names = []

def text_to_numeric(text):
    # Extract numeric part from the text using regular expression
    match = re.search(r'\b(\d+)\b', str(text))
    if match:
        return int(match.group(1))
    else:
        return None

# Loop through PDF files in the directory
for filename in os.listdir(pdf_directory):
    if filename.endswith('.pdf'):
        pdf_path = os.path.join(pdf_directory, filename)

        try:
            # Use tabula to extract tables from the PDF
            tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)

            # Assuming tables[0] contains '2020-21' column and tables[1] contains 'Total Students' column
            total_median_salary = tables[2]['Median salary of\rplaced graduates per\rannum(Amount in\rRs.)'].apply(text_to_numeric).sum()

            # Append the values to the lists
            all_median_salary.append(total_median_salary)
            file_names.append(filename)

        except Exception as e:
            print(f"Error processing {filename}: {e}")

# Create a single DataFrame with 'NT', 'NE', and 'File Name' columns
median_salary_dataframe = pd.DataFrame({'MS': all_median_salary, 'File Name': file_names})

# Display the combined DataFrame
print("median_salary_dataframe :")
median_salary_dataframe



df3 = pd.DataFrame({'GMS': [20, 21.18, 19.5, 21.93, 20.78, 20.73, 22.09, 21.12, 17.02, 15.95, 13.38, 16.87, 11.72, 20.37, 20.27, 14.52, 17.91, 16.71, 10.21, 15.00, 17.42, 18.96, 16.24, 13.54, 19.66, 15.00, 9.54, 9.54, 17.44, 18.42, 10.18, 11.43, 21.25, 12.41, 13.52, 10.62, 14.40, 12.56, 11.15, 14.72, 19.77, 15.58, 13.99, 11.36, 12.25, 13.59, 19.01, 16.27, 16.84, 11.19, 16.62, 8.72, 15.13, 10.23, 25.00, 14.14, 20.82, 12.25, 16.45, 18.80, 13.34, 11.18, 11.25, 8.96, 13.82, 9.30, 17.20, 12.41, 16.15, 14.59, 12.79, 13.33, 14.90, 23.16, 20.00, 11.38, 11.07, 11.65, 13.87, 15.30, 17.39, 13.58, 10.09, 14.04, 10.14, 10.09, 9.08, 21.40, 23.23, 13.34, 12.96, 12.34, 15.54, 11.16, 12.94, 14.69, 15.59, 9.82, 9.24, 10.45]})

# Concatenate the two DataFrames vertically (along rows)
median_salary_dataframe = pd.concat([df3, median_salary_dataframe], axis=1)

"""#Display the median dataframe"""

median_salary_dataframe

from matplotlib import pyplot as plt
median_salary_dataframe['MS'].plot(kind='hist', bins=20, title='NT')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
median_salary_dataframe['GMS'].plot(kind='hist', bins=20, title='GMS')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
median_salary_dataframe.plot(kind='scatter', x='MS', y='GMS', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

"""#Train, validation, test median salary datasets"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Assuming your DataFrame is already defined and contains the necessary columns
# Here, 'NT', 'NE', 'NP', and 'SS' are column names

# Split the data into features (X) and target variable (y)
X = median_salary_dataframe[['MS']]
y = median_salary_dataframe['GMS']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
model = DecisionTreeRegressor()

# Train the model
reg = model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have the actual and predicted values in y_test and y_pred

# Get indices for sorting
indices = np.argsort(y_test)

# Sort actual and predicted values based on y_test
y_test_sorted = np.array(y_test)[indices]
y_pred_sorted = np.array(y_pred)[indices]

# Create an array for the x-axis ticks
x_ticks = np.arange(len(y_test))

# Plotting
plt.figure(figsize=(10, 6))

# Bar plot for actual values
plt.bar(x_ticks - 0.2, y_test_sorted, width=0.4, label='Actual', alpha=0.7)

# Bar plot for predicted values
plt.bar(x_ticks + 0.2, y_pred_sorted, width=0.4, label='Predicted', alpha=0.7)

# Hide x-axis values
plt.xticks([])

# Adding labels and title
# plt.xlabel('Sample Index')
plt.ylabel('Target Variable (GMS)')
plt.title('Actual vs Predicted Values')
plt.legend()

# Show the plot
plt.show()

"""#Calculate Financial Resources and their Utilisation (FRU)"""

import tabula
import os
import pandas as pd


# Loop through PDF files in the directory
for filename in os.listdir(pdf_directory):
    if filename.endswith('.pdf'):
        pdf_path = os.path.join(pdf_directory, filename)

        try:
            # Use tabula to extract tables from the PDF
            tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)

            # Print the first table for inspection
            if tables:
                print(f"First table for {filename}:")
                print(tables[7])
            else:
                print(f"No tables found in {filename}")

            # Continue with your processing logic...

        except Exception as e:
            print(f"Error processing {filename}: {e}")



import tabula

def extract_tables_from_pdf(pdf_path):
    # Read tables from the PDF
    tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)

    # Print the extracted tables
    for i, table in enumerate(tables, start=1):
        print(f"Table {i}:\n{table}\n")

# Example usage
pdf_path = "/content/drive/MyDrive/college_pdf/iit madras.pdf"
extract_tables_from_pdf(pdf_path)

import pandas as pd
import re

all_bo_result = []
all_bc_result = []
file_names = []

def calculate_bc(pdf_path):
    # Read relevant tables from the PDF
    tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)

    # for idx, table in enumerate(tables):
    #   print(idx, "abcdabcd\nabcdabcd\n", table)

    # Extract data from Tables 8, 10
    table8 = tables[7]

    # Extracting Utilised Amounts for the relevant financial years
    bc_values = get_numeric_values(table8, ['2021-22', '2020-21', '2019-20'])

    print(bc_values)

    # Check if there are valid values before calculating the mean
    if bc_values :
        # Calculate FRU
        fru = sum(bc_values)/3
        return fru

    # Return NaN if there are no valid values
    return float('nan')

def calculate_bo(pdf_path):
    # Read relevant tables from the PDF
    tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)

    # for idx, table in enumerate(tables):
    #   print(idx, "abcdabcd\nabcdabcd\n", table)

    # Extract data from Tables 8, 10

    table10 = tables[9]

    # Extracting Utilised Amounts for the relevant financial years
    bo_values = get_numeric_values(table10, ['2021-22', '2020-21', '2019-20'])

    print( bo_values)

    # Check if there are valid values before calculating the mean
    if  bo_values:
        # Calculate FRU
        fru = sum(bo_values)/3
        return fru

    # Return NaN if there are no valid values
    return float('nan')


def text_to_numeric(text):
    # Extract numeric part from the text using regular expression
    match = re.search(r'\b(\d+)\b', str(text))
    if match:
        return int(match.group(1))
    else:
        return None

def get_numeric_values(table, column_labels):
    values = []
    for label in column_labels:
        if label in table.columns:
            # Extract numeric values and exclude 'Utilised Amount' rows
            numeric_column = table[label].iloc[2:].apply(text_to_numeric)
            numeric_column = numeric_column.dropna()
            values += numeric_column.tolist()

    return values


for filename in os.listdir(pdf_directory):
    if filename.endswith('.pdf'):
        pdf_path = os.path.join(pdf_directory, filename)

        try:
            bo_result = calculate_bo(pdf_path)
            bc_result = calculate_bc(pdf_path)
            # Append the values to the lists
            all_bo_result.append(bo_result)
            all_bc_result.append(bc_result)
            file_names.append(filename)

        except Exception as e:
            print(f"Error processing {filename}: {e}")

# Create a single DataFrame with 'NT', 'NE', and 'File Name' columns
bo_bc_dataframe = pd.DataFrame({'BO': all_bo_result, 'BC': all_bc_result, 'File Name': file_names})

# Display the combined DataFrame
print("Combined DataFrame:")
bo_bc_dataframe

fru_df = pd.DataFrame({'FRU': [29.40, 23.21, 20.53, 23.20, 17.74, 14.71, 16.20, 22.89, 16.99, 11.32, 7.16, 18.90, 9.98, 17.23, 14.27, 13.71, 15.19, 23.32, 10.22, 11.81, 14.95, 19.81, 13.33, 17.36, 11.77, 9.77, 13.74, 9.53, 9.57, 21.34, 8.52, 12.90, 15.92, 10.82, 12.83, 10.07, 11.22, 10.11, 11.51, 12.25, 16.27, 13.51, 13.11, 6.30, 10.73, 9.23, 15.30, 19.03, 13.18, 5.39, 13.72, 18.60, 11.19, 11.01, 12.24, 11.64, 16.45, 11.90, 19.00, 8.87, 7.77, 10.55, 7.81, 7.77, 10.90, 7.17, 19.60, 8.58, 17.94, 9.39, 11.50, 13.38, 13.77, 26.04, 12.24, 8.30, 10.52, 8.43, 20.64, 9.15, 19.67, 13.10, 15.62, 8.01, 4.16, 7.69, 4.12, 11.42, 8.90, 11.30, 9.97, 8.70, 16.59, 14.42, 10.31, 7.54, 10.38, 6.87, 7.19, 9.62]})

# Concatenate the two DataFrames vertically (along rows)
fru_dataframe = pd.concat([fru_df, bo_bc_dataframe], axis=1)

fru_dataframe

from tabula import read_pdf

# Read the PDF file
data = read_pdf("/content/drive/MyDrive/college_pdf/iit bombay.pdf")

# Access the extracted tables
for table in data:
    # Print the table data
    print(table)

    # Save the table to a file
    table.to_csv("table_data.csv")

import tabula
import os
import pandas as pd

def calculate_average_expenditure(pdf_path):
    try:
        # Use tabula to extract tables from the PDF
        tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)

        # Iterate over tables
        for idx, table in enumerate(tables):
            # Create a DataFrame
            df = pd.DataFrame(table)

            # Extract numerical values and convert to integers
            for col in df.columns[1:]:
                df[col] = pd.to_numeric(df[col], errors='coerce')

            # Calculate the sum for each Financial Year
            df['Sum'] = df[df.columns[1:]].sum(axis=1)

            # Calculate the average expenditure for each Financial Year
            df['Average'] = df['Sum'] / len(df.columns[1:])

            # Print the result for each table
            print(f"Table {idx + 1} - {pdf_path}")
            print(df[['Financial Year', 'Average']])
            print()

    except Exception as e:
        print(f"Error processing {pdf_path}: {e}")

# Specify the directory containing PDF files
pdf_directory = '/content/drive/MyDrive/college_pdf'

# Loop through PDF files in the directory
for filename in os.listdir(pdf_directory):
    if filename.endswith('.pdf'):
        pdf_path = os.path.join(pdf_directory, filename)
        calculate_average_expenditure(pdf_path)

import pandas as pd
from tabula import read_pdf

# List to store results
average_expenditure_list = []

# List of PDF files
pdf_files = [
    '/content/drive/MyDrive/college_pdf/iit madras.pdf',
    '/content/drive/MyDrive/college_pdf/iit delhi.pdf',
    '/content/drive/MyDrive/college_pdf/iit bombay.pdf',
    '/content/drive/MyDrive/college_pdf/iit kanpur.pdf',
    '/content/drive/MyDrive/college_pdf/iit roorkee.pdf',
    '/content/drive/MyDrive/college_pdf/iit kharagpur.pdf',
    '/content/drive/MyDrive/college_pdf/iit guwahati.pdf',
    '/content/drive/MyDrive/college_pdf/iit hyderabad.pdf'
]

# Process each PDF file
for pdf_path in pdf_files:
    print(f"Processing: {pdf_path}")

    # Extract tables from PDF
    tables = read_pdf(pdf_path, pages='all', multiple_tables=True, pandas_options={'header': None})

    # Process each table
    for idx, table in enumerate(tables):
        # Create a DataFrame
        df = pd.DataFrame(table)

        # Check if 'Financial Year' column is present
        if 'Financial Year' not in df.columns:
            print(f"Skipping table {idx + 1} in {pdf_path}: 'Financial Year' column not found.")
            continue

        # Print the structure of the table for debugging
        print(f"Table {idx + 1} in {pdf_path} structure:")
        print(df)

        # Extract numerical values and convert to integers
        for col in df.columns[1:]:
            df[col] = df[col].str.extract('(\d+)').astype(float)

        # Sum values for each Financial Year
        df['Sum'] = df[df.columns[1:]].sum(axis=1)

        # Calculate the average
        average_expenditure = df['Sum'].mean()

        average_expenditure_list.append({
            'file': pdf_path,
            'page': idx + 1,
            'average_expenditure': average_expenditure
        })

# Display the results
for result in average_expenditure_list:
    print(result)

average_expenditure_list_df = pd.DataFrame(average_expenditure_list)

average_expenditure_list_df.head(20)

resulting_table

import pandas as pd
import tabula

def extract_and_concat_tables(pdf_path, pages, regions):
    all_tables = []

    for page, region in zip(pages, regions):
        # Extract table from the specified region on the page
        tables = tabula.read_pdf(pdf_path, pages=page, area=region)

        # Handle continuity with the previous page's last row
        if all_tables:
            first_table = tables[0]
            last_row_previous = all_tables[-1].iloc[-1]
            first_row_current = first_table.iloc[0]

            # Check if the tables need to be concatenated
            if last_row_previous.equals(first_row_current):
                tables[0] = pd.concat([last_row_previous.to_frame().T, first_table.iloc[1:]])

        # Add the extracted tables to the list
        all_tables.extend(tables)

    # Concatenate all tables into a single DataFrame
    consolidated_table = pd.concat(all_tables, ignore_index=True)

    return consolidated_table

# Example usage:
pdf_path = '/content/drive/MyDrive/college_pdf/iit madras.pdf'
pages_to_extract = [ 2, 3]  # Specify the pages with tables
regions_to_extract = [(0, 1, 500, 100 )]  # Specify the region of each table on the respective page

resulting_table = extract_and_concat_tables(pdf_path, pages_to_extract, regions_to_extract)

import os
import pdfplumber

pdf_file_path = "/content/drive/MyDrive/college_pdf/iit bombay.pdf"
with pdfplumber.open(pdf_file_path) as pdf:

  first_page = pdf.pages[0]
  print (first_page)
  second_page = pdf.pages[1]
  third_page = pdf.pages[2]
  text1 = first_page.extract_text()
  print(f"Text from {pdf_file_path}:\n{text1}")
  #text2 = second_page.extract_text()
  #print(f"Text from {pdf_file_path}:\n{text2}")
  #text3 = third_page.extract_text()
  #print(f"Text from {pdf_file_path}:\n{text3}")

import os
import pdfplumber
extracted_data_nphd = []
pdf_file_path = "/content/drive/MyDrive/college_pdf/iit bombay.pdf"
with pdfplumber.open(pdf_file_path) as pdf:
       # Extract text from the first three pages
       for page_number in range(1, 4):
           current_page = pdf.pages[page_number - 1]
           text = current_page.extract_text()
           # Example: Extracting total students from the text
           match = re.search(r'No\. of Ph\.D students graduated \(including Integrated Ph\.D\)\n2021-22 2020-21 2019-20\nFull Time (\d+) (\d+) (\d+)\nPart Time (\d+) (\d+) (\d+)', text)
           if match:
             full_time_students = [int(match.group(i)) for i in range(1, 4)]
             part_time_students = [int(match.group(i)) for i in range(4, 7)]
             print('Full Time:', full_time_students)
             print('Part Time:', part_time_students)

full_time = sum(full_time_students)
full_time

import re

text = """
No. of Ph.D students graduated (including Integrated Ph.D)
2021-22 2020-21 2019-20
Full Time 416 122 205
Part Time 26 150 66
"""

match = re.search(r'No\. of Ph\.D students graduated \(including Integrated Ph\.D\)\n2021-22 2020-21 2019-20\nFull Time (\d+) (\d+) (\d+)\nPart Time (\d+) (\d+) (\d+)', text)

if match:
    full_time_students = [int(match.group(i)) for i in range(1, 4)]
    part_time_students = [int(match.group(i)) for i in range(4, 7)]
    print('Full Time:', full_time_students)
    print('Part Time:', part_time_students)

full_time = sum(full_time_students)/3

full_time

"""#Metric for Number of Ph.D Students Graduated (GPHD)"""

import os
import pdfplumber
import re
import pandas as pd

# Mount Google Drive
#from google.colab import drive
#drive.mount('/content/drive')

# Navigate to the folder where your PDF files are located
pdf_folder_path = '/content/drive/MyDrive/college_pdf'
os.chdir(pdf_folder_path)

# List PDF files
pdf_files = [file for file in os.listdir() if file.endswith('.pdf')]

# Initialize a list to store extracted data
extracted_data_nphd = []

# Access and process PDF files
for pdf_file_path in pdf_files:
    with pdfplumber.open(pdf_file_path) as pdf:
        # Extract text from the first three pages
        for page_number in range(1, 4):
            current_page = pdf.pages[page_number - 1]
            text = current_page.extract_text()
            # Example: Extracting total students from the text
            match = re.search(r'No\. of Ph\.D students graduated \(including Integrated Ph\.D\)\n2021-22 2020-21 2019-20\nFull Time (\d+) (\d+) (\d+)\nPart Time (\d+) (\d+) (\d+)', text)
            if match:
              full_time_students = [int(match.group(i)) for i in range(1, 4)]
              part_time_students = [int(match.group(i)) for i in range(4, 7)]
              full_time = sum(full_time_students)/3
              part_time = sum(part_time_students)/3
              extracted_data_nphd.append({
                  'file': pdf_file_path,
                  'page': page_number,
                  'full_time_students': full_time,
                  'part_time_students': part_time
              })

# Print the extracted data
print(extracted_data_nphd)

# Create a pandas DataFrame from the extracted data
df4 = pd.DataFrame(extracted_data_nphd)

# Print the DataFrame
print(df4)

df4.head(20)

new_row = {
                  'file': 'iit kharagpur.pdf',
                  'page': 2,
                  'full_time_students': 361,
                  'part_time_students': 0
              }

df4.loc[4.5] = new_row
df4 = df4.sort_index().reset_index(drop=True)

len(df4)

nphd_df = pd.DataFrame({'NPHD': [16.75, 16.73, 17.20, 16.02, 15.54, 20.00, 15.61, 8.43, 11.57, 12.06, 14.11, 11.49, 12.25, 9.84, 11.51, 11.61, 11.22, 7.91, 2.28, 9.28, 9.18, 5.54, 10.21, 9.70, 6.76, 7.35, 8.31, 7.82, 5.09, 3.97, 5.73, 5.66, 7.45, 6.82, 10.72, 8.00, 8.08, 7.94, 10.44, 6.79, 8.34, 7.12, 9.38, 7.29, 6.22, 6.44, 6.41, 2.95, 9.00, 2.79, 2.22, 1.22, 4.92, 0.52, 2.34, 5.73, 2.62, 4.21, 0.38, 7.03, 3.83, 3.06, 3.06, 1.15, 6.37, 5.46, 0.23, 8.23, 0.15, 6.69, 5.70, 2.16, 1.76, 0.88, 3.38, 0.81, 0.74, 1.98, 2.51, 3.83, 0.59, 5.73, 3.12, 3.27, 2.95, 2.68, 0.52, 1.41, 4.84, 2.51, 4.57, 1.22, 0.15, 7.19, 2.90, 1.02, 4.21, 0.38, 1.98, 0.23]})

# Concatenate the two DataFrames vertically (along rows)
nphd_df = pd.concat([df4, nphd_df], axis=1)

#df4.drop_duplicates()

# Sum the columns using the + operator
nphd_df['total_students'] = nphd_df['full_time_students'] + nphd_df['part_time_students']

"""#Display the phd students datagrame"""

nphd_df

from matplotlib import pyplot as plt
nphd_df['NPHD'].plot(kind='hist', bins=20, title='NPHD')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
nphd_df.plot(kind='scatter', x='total_students', y='NPHD', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

"""#Train, validate, test Datasets"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Assuming your DataFrame is already defined and contains the necessary columns
# Here, 'NT', 'NE', 'NP', and 'SS' are column names

# Split the data into features (X) and target variable (y)
X = nphd_df[['total_students']]
y = nphd_df['NPHD']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
model = LinearRegression()

# Train the model
reg = model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have the actual and predicted values in y_test and y_pred

# Get indices for sorting
indices = np.argsort(y_test)

# Sort actual and predicted values based on y_test
y_test_sorted = np.array(y_test)[indices]
y_pred_sorted = np.array(y_pred)[indices]

# Create an array for the x-axis ticks
x_ticks = np.arange(len(y_test))

# Plotting
plt.figure(figsize=(10, 6))

# Bar plot for actual values
plt.bar(x_ticks - 0.2, y_test_sorted, width=0.4, label='Actual', alpha=0.7)

# Bar plot for predicted values
plt.bar(x_ticks + 0.2, y_pred_sorted, width=0.4, label='Predicted', alpha=0.7)

# Hide x-axis values
plt.xticks([])

# Adding labels and title
# plt.xlabel('Sample Index')
plt.ylabel('Target Variable (NPHD)')
plt.title('Actual vs Predicted Values')
plt.legend()

# Show the plot
plt.show()

"""# Calculate Economically and Socially Challenged Students (ESCS)"""

# Initialize lists to store total values and file names
all_nesc_values = []
escs_file_names = []

# Loop through PDF files in the directory
for filename in os.listdir(pdf_directory):
    if filename.endswith('.pdf'):
        pdf_path = os.path.join(pdf_directory, filename)

        try:
            # Use tabula to extract tables from the PDF
            tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)

            # Assuming tables[0] contains '2020-21' column and tables[1] contains 'Total Students' column
            students_tution = tables[1]['No. of students\rreceiving full\rtuition fee\rreimbursement\rfrom Institution\rFunds'].sum()
            total_students = tables[1]['Total Students'].sum()
            nesc_value = (students_tution/total_students)*100

            # Append the values to the lists
            all_nesc_values.append(nesc_value)
            escs_file_names.append(filename)

        except Exception as e:
            print(f"Error processing {filename}: {e}")

# Create a single DataFrame with 'NT', 'NE', and 'File Name' columns
escs_dataframe = pd.DataFrame({'NESC': all_nesc_values, 'File Name': escs_file_names})

# Display the combined DataFrame
print("Combined DataFrame:")
escs_dataframe

escs_df = pd.DataFrame({'ESCS': [7.86, 8.33, 4.41, 7.53, 5.38, 5.88, 0.95, 3.46, 8.41, 8.24, 0.12, 9.99, 4.14, 5.76, 9.17, 8.69, 7.76, 4.71, 5.44, 0.79, 4.73, 7.91, 9.76, 1.10, 0.02, 9.16, 1.30, 0.80, 3.19, 5.42, 0.16, 3.23, 9.31, 0.44, 9.83, 7.80, 7.50, 4.56, 1.86, 4.02, 7.02, 3.48, 8.16, 0.48, 3.81, 8.38, 6.48, 4.38, 9.82, 0.00, 11.1, 3.56, 1.01, 0.07, 0.43, 10.03, 0.00, 3.46, 9.70, 2.36, 1.51, 0.43, 2.12, 6.17, 3.58, 2.28, 5.35, 4.49, 4.79, 8.41, 0.00, 9.37, 5.50, 0.00, 0.67, 0.19, 3.17, 1.73, 3.36, 10.01, 7.22, 12.68, 6.24, 1.21, 5.86, 9.08, 6.27, 3.44, 3.61, 2.74, 8.66, 0.41, 2.47, 6.19, 10.14, 2.29, 6.09, 6.36, 2.83, 5.64]})

# Concatenate the two DataFrames vertically (along rows)
escs_df = pd.concat([escs_df, escs_dataframe], axis=1)

"""# Display relationship between ESCS vs NESC"""

escs_df

from matplotlib import pyplot as plt
escs_df['NESC'].plot(kind='hist', bins=20, title='NESC')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
escs_df.plot(kind='scatter', x='NESC', y='ESCS', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

"""#Train, test datasets"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Assuming your DataFrame is already defined and contains the necessary columns
# Here, 'NT', 'NE', 'NP', and 'SS' are column names

# Split the data into features (X) and target variable (y)
X = escs_df[['NESC']]
y = escs_df['ESCS']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
model = LinearRegression()

# Train the model
reg = model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have the actual and predicted values in y_test and y_pred

# Get indices for sorting
indices = np.argsort(y_test)

# Sort actual and predicted values based on y_test
y_test_sorted = np.array(y_test)[indices]
y_pred_sorted = np.array(y_pred)[indices]

# Create an array for the x-axis ticks
x_ticks = np.arange(len(y_test))

# Plotting
plt.figure(figsize=(10, 6))

# Bar plot for actual values
plt.bar(x_ticks - 0.2, y_test_sorted, width=0.4, label='Actual', alpha=0.7)

# Bar plot for predicted values
plt.bar(x_ticks + 0.2, y_pred_sorted, width=0.4, label='Predicted', alpha=0.7)

# Hide x-axis values
plt.xticks([])

# Adding labels and title
# plt.xlabel('Sample Index')
plt.ylabel('Target Variable (ESCS)')
plt.title('Actual vs Predicted Values')
plt.legend()

# Show the plot
plt.show()